简单加权融合:
回归（分类概率）：算术平均融合（Arithmetic mean），几何平均融合（Geometric mean）；
分类：投票（Voting)
综合：排序融合(Rank averaging)，log融合
stacking/blending:
构建多层模型，并利用预测结果再拟合预测。
boosting/bagging（在xgboost，Adaboost,GBDT中已经用到）:
多树的提升方法

stacking:
简单来说 stacking 就是当用初始训练数据学习出若干个基学习器后，将这几个学习器的预测结果作为新的训练集，来学习一个新的学习器。
Step 1. 基模型 Model1_1，对训练集train训练，然后用于预测 train 和 test 的标签列，分别是P1，T1
Step 2. 基模型 Model1_2 ，对训练集train训练，然后用于预测train和test的标签列，分别是P2，T2
Step 3. 分别把P1,P2以及T1,T2合并，得到一个新的训练集和测试集train2,test2.

比赛的融合这个问题，个人的看法来说其实涉及多个层面，也是提分和提升模型鲁棒性的一种重要方法：
1）结果层面的融合，这种是最常见的融合方法，其可行的融合方法也有很多，比如根据结果的得分进行加权融合，还可以做Log，exp处理等。在做结果融合的时候，
有一个很重要的条件是模型结果的得分要比较近似，然后结果的差异要比较大，这样的结果融合往往有比较好的效果提升。
2）特征层面的融合，这个层面其实感觉不叫融合，准确说可以叫分割，很多时候如果我们用同种模型训练，可以把特征进行切分给不同的模型，
然后在后面进行模型或者结果融合有时也能产生比较好的效果。
3）模型层面的融合，模型层面的融合可能就涉及模型的堆叠和设计，比如加Staking层，部分模型的结果作为特征输入等，这些就需要多实验和思考了，
基于模型层面的融合最好不同模型类型要有一定的差异，用同种模型不同的参数的收益一般是比较小的。
